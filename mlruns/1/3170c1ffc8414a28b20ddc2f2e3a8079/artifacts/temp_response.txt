Gradient descent is an iterative optimization algorithm that is used to find local minima or local maxima in a function. It is based on the fact that the gradient of the loss function (i.e., the derivative of the objective function with respect to the parameters) changes according to the direction of minimization or maximization. Gradient descent works by updating each parameter using a small step size, which changes in the direction opposite to the gradient. This process is repeated until a stationary point is reached or an acceptable number of iterations are completed. The algorithm gradually decreases the objective function value towards the global minimum or maximum.